# **Non-Fiction AI Library**

## **Project Overview**  

The Non-Fiction AI Library is an advanced tool designed for extracting and organizing knowledge from PDF documents. It enables users to upload PDFs, extract structured features, and use a **Graph RAG** to learn knowledge concepts. The project leverages **Python**, **FastAPI**, and several key technologies, including **LLMSherpa, MongoDB (pymongo, motor), Neo4j, Ollama, and scikit-learn**.  

Users can interact with the system to:  
- Extract structured knowledge from PDFs.  
- Build a knowledge graph using **Neo4j**.  
- Query the graph for relevant books, chapters, and knowledge concepts.  
- Retrieve and generate insights using **LLMs**.  

## **Technology Stack**  

- **Backend:** FastAPI, LLMSherpa, MongoDB (Motor), Neo4j, Ollama, scikit-learn  
- **Frontend:** Next.js  
- **Database:** MongoDB (for document storage), Neo4j (for knowledge graph)  
- **AI & ML:** LLMSherpa for text extraction, Ollama for embeddings and generation 
- **Containerization:** Docker, Docker Compose  
- **Authentication & Security:** To be implemented  

## **Development Roadmap**  

The project is in the early development phase and follows an iterative roadmap:  

### **Phase 1: Core Feature Development**  
âœ… Backend module for **PDF upload and feature extraction**  
âœ… Backend module for **building the knowledge graph (Neo4j)**  
ðŸ”² Backend module for **querying the knowledge graph using Cypher**  

### **Phase 2: User Interface & Interaction**  
ðŸ”² Frontend module for **PDF upload & display**  
ðŸ”² Frontend module for **querying knowledge concepts, LLM chat & visualizing the knowledge graph**  
ðŸ”² Implement authentication and authorization  

### **Phase 3: Optimization & Production Readiness**  
ðŸ”² Enhance **query performance and caching**  
ðŸ”² Implement **task queues**  
ðŸ”² Improve **scalability & security for production deployment**  
ðŸ”² Deploy a **fully containerized system with optimized networking**  

## **Project Structure**  

The repository is structured as follows:  

```python
ai-library/
â”œâ”€â”€ README.md             # Project documentation and overview
â”œâ”€â”€ api/                  # API-related code and configurations
â”‚   â”œâ”€â”€ .dockerignore     # Files to ignore in Docker builds
â”‚   â”œâ”€â”€ .env              # Environment variables for development
â”‚   â”œâ”€â”€ Dockerfile        # Instructions for building the Docker image
â”‚   â”œâ”€â”€ Makefile          # Commands for building and managing the project
â”‚   â”œâ”€â”€ docker-compose.dev.yml # Configuration for Docker Compose in development
â”‚   â”œâ”€â”€ example.env       # Example environment variables
â”‚   â”œâ”€â”€ pyproject.toml    # Project metadata and dependencies for Python
â”‚   â”œâ”€â”€ pytest.ini        # Configuration for pytest
â”‚   â”œâ”€â”€ src/              # Source code for the API
â”‚   â”‚   â”œâ”€â”€ __init__.py   # Marks the directory as a Python package
â”‚   â”‚   â”œâ”€â”€ config/       # Configuration files for the application
â”‚   â”‚   â”œâ”€â”€ database/     # Database models and migrations
â”‚   â”‚   â”œâ”€â”€ docs/         # Documentation files
â”‚   â”‚   â”œâ”€â”€ main.py       # Main entry point for the API
â”‚   â”‚   â”œâ”€â”€ models/       # Data models for the application
â”‚   â”‚   â”œâ”€â”€ pdf_uploads/  # Directory for uploaded PDF files
â”‚   â”‚   â”œâ”€â”€ repository/    # Data access layer for database interactions
â”‚   â”‚   â”œâ”€â”€ routers/      # API route definitions
â”‚   â”‚   â”œâ”€â”€ schemas/      # Data validation schemas
â”‚   â”‚   â”œâ”€â”€ services/     # Business logic of the application
â”‚   â”‚   â”œâ”€â”€ tasks.py      # Background tasks
â”‚   â”‚   â”œâ”€â”€ utils/        # Utility functions and helpers
â”‚   â”‚   â”œâ”€â”€ worker.py     # Worker for background processing
â”‚   â”œâ”€â”€ tests/            # Test cases for the API
â”œâ”€â”€ frontend/             # Frontend application code
```

## **Key Features**  

- **Intelligent PDF feature extraction** using **LLMSherpa**  
- **Automated clustering** of similar content with **scikit-learn**  
- **Knowledge graph construction** with **Neo4j**  
- **Graph-based retrieval** using Cypher queries  
- **FastAPI backend** with structured API routes  
- **Asynchronous processing** with `asyncio`  
- **Modern frontend** powered by **Next.js**   

### **Production Considerations**  

For a production-grade deployment, it is recommended to:  
- Deploy **each service in a separate container** for scalability.  
- Use **managed database services** for MongoDB and Neo4j.  
- Secure **authentication & authorization mechanisms**.  
- Implement **monitoring, and fault tolerance**.  
- Replace local PDF file storage to AWS S3

## **How to Run Locally**  

1. **Clone the repository:**  

   ```sh
   git clone https://github.com/AlekseiMikhalev/ai-library.git
   cd ai-library
   ```

2. **Set up environment variables:**  
   - Copy `example.env` to `.env` and configure necessary variables.  

3. **Start the development environment:**  

   ```sh
   make start-dev
   ```

4. **Access the services:**  
   - **API**: `http://localhost:8000/docs`  
   - **Frontend**: `http://localhost:3000`  
   - **Neo4j Browser**: `http://localhost:7474`  
   - **MongoDB Compass**: `http://localhost:27017`